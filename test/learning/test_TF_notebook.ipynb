{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing `TFNoiseAwareModel`\n",
    "\n",
    "We'll start by testing the `textRNN` model on a categorical problem from `tutorials/crowdsourcing`.  In particular we'll test for (a) basic performance and (b) proper construction / re-construction of the TF computation graph both after (i) repeated notebook calls, and (ii) with `GridSearch` in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['SNORKELDB'] = 'sqlite:///{0}{1}crowdsourcing.db'.format(os.getcwd(), os.sep)\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load candidates and training marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.contrib.models.text import RawText\n",
    "Tweet = candidate_subclass('Tweet', ['tweet'], cardinality=5)\n",
    "train_tweets = session.query(Tweet).filter(Tweet.split == 0).order_by(Tweet.id).all()\n",
    "len(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, train_tweets, split=0)\n",
    "train_marginals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 6.1 s, sys: 114 ms, total: 6.22 s\n",
      "Wall time: 11.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<568x3526 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 8126 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple unigram featurizer\n",
    "def get_unigram_tweet_features(c):\n",
    "    for w in c.tweet.text.split():\n",
    "        yield w, 1\n",
    "\n",
    "# Construct feature matrix\n",
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator(f=get_unigram_tweet_features)\n",
    "\n",
    "%time F_train = featurizer.apply(split=0)\n",
    "F_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1.02 s, sys: 36.6 ms, total: 1.06 s\n",
      "Wall time: 1.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<64x3526 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 539 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time F_test = featurizer.apply_existing(split=1)\n",
    "F_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression] Training model\n",
      "[LogisticRegression] n_train=568  #epochs=25  batch size=256\n",
      "[LogisticRegression] Epoch 0 (0.06s)\tAverage loss=1.609304\n",
      "[LogisticRegression] Epoch 5 (0.16s)\tAverage loss=0.533771\n",
      "[LogisticRegression] Epoch 10 (0.25s)\tAverage loss=0.269326\n",
      "[LogisticRegression] Epoch 15 (0.36s)\tAverage loss=0.169991\n",
      "[LogisticRegression] Epoch 20 (0.50s)\tAverage loss=0.119820\n",
      "[LogisticRegression] Epoch 24 (0.62s)\tAverage loss=0.099092\n",
      "[LogisticRegression] Training done (0.62s)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(cardinality=Tweet.cardinality)\n",
    "model.train(F_train.todense(), train_marginals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train `SparseLogisticRegression`\n",
    "\n",
    "Note: Testing doesn't currently work with `LogisticRegression` above, but no real reason to use that over this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=568  #epochs=50  batch size=256\n",
      "[SparseLogisticRegression] Epoch 0 (0.06s)\tAverage loss=1.656555\n",
      "[SparseLogisticRegression] Epoch 10 (0.40s)\tAverage loss=0.278991\n",
      "[SparseLogisticRegression] Epoch 20 (0.79s)\tAverage loss=0.130961\n",
      "[SparseLogisticRegression] Epoch 30 (1.41s)\tAverage loss=0.080214\n",
      "[SparseLogisticRegression] Epoch 40 (1.75s)\tAverage loss=0.058399\n",
      "[SparseLogisticRegression] Epoch 49 (2.07s)\tAverage loss=0.048295\n",
      "[SparseLogisticRegression] Training done (2.07s)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "\n",
    "model = SparseLogisticRegression(cardinality=Tweet.cardinality)\n",
    "model.train(F_train, train_marginals, n_epochs=50, print_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_labels = np.load('crowdsourcing_test_labels.npy')\n",
    "acc = model.score(F_test, test_labels)\n",
    "print acc\n",
    "assert acc > 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train basic LSTM\n",
    "\n",
    "With dev set scoring during execution (note we use test set here to be simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextRNN] Training model\n",
      "[TextRNN] n_train=568  #epochs=100  batch size=256\n",
      "[TextRNN] Epoch 0 (0.67s)\tAverage loss=1.602129\tDev Acc.=40.62\n",
      "[TextRNN] Epoch 5 (4.24s)\tAverage loss=1.342293\tDev Acc.=34.38\n",
      "[TextRNN] Epoch 10 (7.79s)\tAverage loss=0.730269\tDev Acc.=53.12\n",
      "[TextRNN] Epoch 15 (11.03s)\tAverage loss=0.312139\tDev Acc.=68.75\n",
      "[TextRNN] Epoch 20 (14.36s)\tAverage loss=0.139608\tDev Acc.=62.50\n",
      "[TextRNN] Epoch 25 (17.71s)\tAverage loss=0.086493\tDev Acc.=65.62\n",
      "[TextRNN] Epoch 30 (20.95s)\tAverage loss=0.080443\tDev Acc.=70.31\n",
      "[TextRNN] Epoch 35 (24.26s)\tAverage loss=0.045322\tDev Acc.=67.19\n",
      "[TextRNN] Epoch 40 (27.59s)\tAverage loss=0.050468\tDev Acc.=68.75\n",
      "[TextRNN] Epoch 45 (30.87s)\tAverage loss=0.045916\tDev Acc.=70.31\n",
      "[TextRNN] Epoch 50 (34.17s)\tAverage loss=0.031020\tDev Acc.=68.75\n",
      "[TextRNN] Epoch 55 (37.50s)\tAverage loss=0.026304\tDev Acc.=67.19\n",
      "[TextRNN] Epoch 60 (41.24s)\tAverage loss=0.030072\tDev Acc.=67.19\n",
      "[TextRNN] Epoch 65 (45.06s)\tAverage loss=0.021733\tDev Acc.=65.62\n",
      "[TextRNN] Epoch 70 (48.94s)\tAverage loss=0.024205\tDev Acc.=65.62\n",
      "[TextRNN] Epoch 75 (52.62s)\tAverage loss=0.019788\tDev Acc.=65.62\n",
      "[TextRNN] Epoch 80 (55.97s)\tAverage loss=0.023895\tDev Acc.=65.62\n",
      "[TextRNN] Model saved as <TextRNN>\n",
      "[TextRNN] Epoch 85 (61.78s)\tAverage loss=0.017011\tDev Acc.=65.62\n",
      "[TextRNN] Epoch 90 (65.45s)\tAverage loss=0.018782\tDev Acc.=65.62\n",
      "[TextRNN] Epoch 95 (69.54s)\tAverage loss=0.019424\tDev Acc.=65.62\n",
      "[TextRNN] Epoch 99 (72.34s)\tAverage loss=0.019783\tDev Acc.=65.62\n",
      "[TextRNN] Training done (72.39s)\n",
      "[TextRNN] Loaded model <TextRNN>\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import TextRNN\n",
    "test_tweets = session.query(Tweet).filter(Tweet.split == 1).order_by(Tweet.id).all()\n",
    "\n",
    "train_kwargs = {\n",
    "    'dim':        100,\n",
    "    'lr':         0.001,\n",
    "    'n_epochs':   100,\n",
    "    'dropout':    0.2,\n",
    "    'print_freq': 5\n",
    "}\n",
    "lstm = TextRNN(seed=1701, cardinality=Tweet.cardinality)\n",
    "lstm.train(train_tweets, train_marginals, X_dev=test_tweets, Y_dev=test_labels, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65625\n"
     ]
    }
   ],
   "source": [
    "acc = lstm.score(test_tweets, test_labels)\n",
    "print acc\n",
    "assert acc > 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `GridSearch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing lr = 1.00e-04, dim = 50\n",
      "============================================================\n",
      "[TextRNN] Training model\n",
      "[TextRNN] n_train=568  #epochs=50  batch size=256\n",
      "[TextRNN] Epoch 0 (0.42s)\tAverage loss=1.610982\n",
      "[TextRNN] Epoch 10 (3.61s)\tAverage loss=1.598022\n",
      "[TextRNN] Epoch 20 (6.78s)\tAverage loss=1.584193\n",
      "[TextRNN] Epoch 30 (9.94s)\tAverage loss=1.564177\n",
      "[TextRNN] Epoch 40 (13.11s)\tAverage loss=1.532403\n",
      "[TextRNN] Epoch 49 (15.96s)\tAverage loss=1.472197\n",
      "[TextRNN] Training done (15.96s)\n",
      "[TextRNN] Accuracy: 0.40625\n",
      "[TextRNN] Model saved as <TextRNN_0>\n",
      "============================================================\n",
      "[2] Testing lr = 1.00e-04, dim = 100\n",
      "============================================================\n",
      "[TextRNN] Training model\n",
      "[TextRNN] n_train=568  #epochs=50  batch size=256\n",
      "[TextRNN] Epoch 0 (0.67s)\tAverage loss=1.610511\n",
      "[TextRNN] Epoch 10 (7.84s)\tAverage loss=1.579410\n",
      "[TextRNN] Epoch 20 (15.12s)\tAverage loss=1.541513\n",
      "[TextRNN] Epoch 30 (22.31s)\tAverage loss=1.473694\n",
      "[TextRNN] Epoch 40 (29.79s)\tAverage loss=1.301941\n",
      "[TextRNN] Epoch 49 (36.58s)\tAverage loss=1.073563\n",
      "[TextRNN] Training done (36.58s)\n",
      "[TextRNN] Accuracy: 0.421875\n",
      "[TextRNN] Model saved as <TextRNN_1>\n",
      "============================================================\n",
      "[3] Testing lr = 1.00e-03, dim = 50\n",
      "============================================================\n",
      "[TextRNN] Training model\n",
      "[TextRNN] n_train=568  #epochs=50  batch size=256\n",
      "[TextRNN] Epoch 0 (0.35s)\tAverage loss=1.607748\n",
      "[TextRNN] Epoch 10 (3.18s)\tAverage loss=1.308417\n",
      "[TextRNN] Epoch 20 (6.07s)\tAverage loss=0.448450\n",
      "[TextRNN] Epoch 30 (9.13s)\tAverage loss=0.121402\n",
      "[TextRNN] Epoch 40 (12.18s)\tAverage loss=0.094715\n",
      "[TextRNN] Epoch 49 (14.82s)\tAverage loss=0.046768\n",
      "[TextRNN] Training done (14.82s)\n",
      "[TextRNN] Accuracy: 0.578125\n",
      "[TextRNN] Model saved as <TextRNN_2>\n",
      "============================================================\n",
      "[4] Testing lr = 1.00e-03, dim = 100\n",
      "============================================================\n",
      "[TextRNN] Training model\n",
      "[TextRNN] n_train=568  #epochs=50  batch size=256\n",
      "[TextRNN] Epoch 0 (0.63s)\tAverage loss=1.602129\n",
      "[TextRNN] Epoch 10 (7.25s)\tAverage loss=0.730269\n",
      "[TextRNN] Epoch 20 (13.68s)\tAverage loss=0.139608\n",
      "[TextRNN] Epoch 30 (19.98s)\tAverage loss=0.080443\n",
      "[TextRNN] Epoch 40 (26.45s)\tAverage loss=0.050468\n",
      "[TextRNN] Epoch 49 (32.31s)\tAverage loss=0.030446\n",
      "[TextRNN] Training done (32.31s)\n",
      "[TextRNN] Accuracy: 0.6875\n",
      "[TextRNN] Model saved as <TextRNN_3>\n",
      "[TextRNN] Loaded model <TextRNN_3>\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.utils import GridSearch, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-4, 1e-3, step=1, log_base=10)\n",
    "dim_param = RangeParameter('dim', 50, 100, step=50)\n",
    "\n",
    "searcher = GridSearch(TextRNN, [rate_param, dim_param], train_tweets, train_marginals,\n",
    "                     seed=1701, cardinality=Tweet.cardinality)\n",
    "\n",
    "# Use test set here (just for testing)\n",
    "train_kwargs = {\n",
    "    'dim':        100,\n",
    "    'n_epochs':   50,\n",
    "    'dropout':    0.2,\n",
    "    'print_freq': 10\n",
    "}\n",
    "\n",
    "lstm, run_stats = searcher.fit(test_tweets, test_labels, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "acc = lstm.score(test_tweets, test_labels)\n",
    "print acc\n",
    "assert acc > 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload saved model outside of `GridSearch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextRNN] Loaded model <TextRNN_3>\n",
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "lstm = TextRNN(seed=1701, cardinality=Tweet.cardinality)\n",
    "lstm.load('TextRNN_3')\n",
    "acc = lstm.score(test_tweets, test_labels)\n",
    "print acc\n",
    "assert acc > 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload a model with different structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextRNN] Loaded model <TextRNN_0>\n",
      "0.40625\n"
     ]
    }
   ],
   "source": [
    "lstm.load('TextRNN_0')\n",
    "acc = lstm.score(test_tweets, test_labels)\n",
    "print acc\n",
    "assert acc < 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing `GenerativeModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `GridSearch` on crowdsourcing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_label_matrix\n",
    "import numpy as np\n",
    "\n",
    "L_train = load_label_matrix(session, split=0)\n",
    "train_labels = np.load('crowdsourcing_train_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing epochs = 0\n",
      "============================================================\n",
      "Inferred cardinality: 5\n",
      "[GenerativeModel] Accuracy: 0.984154929577\n",
      "[GenerativeModel] Model saved as <GenerativeModel_0>.\n",
      "============================================================\n",
      "[2] Testing epochs = 10\n",
      "============================================================\n",
      "Inferred cardinality: 5\n",
      "[GenerativeModel] Accuracy: 0.991197183099\n",
      "[GenerativeModel] Model saved as <GenerativeModel_1>.\n",
      "============================================================\n",
      "[3] Testing epochs = 30\n",
      "============================================================\n",
      "Inferred cardinality: 5\n",
      "[GenerativeModel] Accuracy: 0.996478873239\n",
      "[GenerativeModel] Model saved as <GenerativeModel_2>.\n",
      "[GenerativeModel] Model <GenerativeModel_2> loaded.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning.utils import ListParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "n_epochs = ListParameter('epochs', [0, 10, 30])\n",
    "searcher = GridSearch(GenerativeModel, [n_epochs], L_train)\n",
    "\n",
    "# Use training set labels here (just for testing)\n",
    "gen_model, run_stats = searcher.fit(L_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996478873239\n"
     ]
    }
   ],
   "source": [
    "acc = gen_model.score(L_train, train_labels)\n",
    "print acc\n",
    "assert acc > 0.97"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
